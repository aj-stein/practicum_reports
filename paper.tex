\documentclass{jdf}

\begin{document}
Section: PUBP-6727
\title{Mutual Monitoring in the Cloud \\ Progress Report 2}
\author{Alexander Stein}

\maketitle
\thispagestyle{fancy}


\section*{Problem Statement}

Cloud computing infrastructure is essentially ubiquitous, but adoption is not without challenges. Cloud service providers must cater to customers in regulated sectors, complying with cybersecurity frameworks that create high barriers to entry. One barrier is ongoing evaluation of the provider's cybersecurity posture, often resulting in centralized bureaucracies. FedRAMP oversees a prominent example of such a program, the Continuous Monitoring Program, which is emblematic of these barriers. This program requires hundreds of cloud service providers to contract with one of thirty reputable auditor firms. The providers work with the auditors to send security scans and updated security control documentation for FedRAMP-authorized services monthly to FedRAMP reviewers, in some cases for the largest cloud infrastructures in the world. All three parties collaborate in meetings, emails, and a wiki, forming a unique multi-party bureaucracy that both secures and bottlenecks the government's acquisition of modern cloud services.

Are these bureaucracies an optimal solution, or a last resort that fails to keep pace with cloud technology as it proliferates and evolves? If they are a last resort, is there a better way?

\section*{Solution Statement}

I will use this research to design and evaluate an alternative to centralized continuous monitoring, mutual monitoring. The foundation of mutual monitoring will be federated data services, known in other security use cases as \hyperlink{https://transparency.dev}{transparency services}. The positives and negatives of FedRAMP's continuous monitoring model will inform its design. Operating such services can change the economics, and thereby the behavior, of cloud service providers and their customers. A new architecture will incentivize auditors to sell value-add analytics via these federated data services, potentially obsoleting centralized authorities for continuous monitoring like FedRAMP.

\section*{Completed Tasks (Last 2 Weeks)}

\begin{enumerate}
    \item I developed a build workflow and automated publication pipeline for the \hyperlink{https://add-architecture-draft--conmotion.netlify.app/architecture.html}{draft architecture specification}. Fixing bugs and troubleshooting took more time than I estimated in my plan.
        \begin{itemize}
            \item I evaluated \hyperlink{https://github.com/martinthomson/i-d-template}{the use of software for authoring specifications popular with IETF standards authors}.
            \item Due to complexity of modifying the aforementioned tooling to remove IETF branding, copyright notices, styling, and features not suitable to my specification, I designed a workflow to author and manage drafts of the specification alongside \hyperlink{https://github.com/aj-stein/conmotion/}{the same source code repository for the prototype} using the open-source \hyperlink{https://pandoc.org/}{pandoc utility}.
            \item I developed \hyperlink{https://github.com/aj-stein/conmotion/blob/develop/.github/workflows/cd.yml}{an automation pipeline to execute the publication workflow for the specification remotely in GitHub Actions} to make the authoring process more consistent over time.
            \item I created and completed troubleshooting on a deployment pipeline for \hyperlink{https://aj-stein.github.io/conmotion/}{final complete copy} and \hyperlink{https://conmotion.netlify.app/}{incremental draft copies} of the specification. The latter is helpful to streamline feedback with multiple advisors with different copies of changes in parallel. I will integrate this work into managing later deployments of the transparency service prototypes, greatly improving productivity. 
            \item Due to the custom needs of how I deployed pandoc, I created a \hyperlink{https://github.com/users/aj-stein/packages/container/package/pandoc\%2Flatex}{customized container image} for building the website and a PDF copy.
            \item I extended my publication workflow to generate images from architecture diagrams in \hyperlink{https://mermaid.js.org/}{the Mermaid family of declarative domain-specific languages for diagrams}.
            \item I extended my publication workflow to automatically convert the PDF copy and cross-link to it on the web version to suit the preferences of several of my advisors.
        \end{itemize}
    \item I began writing \hyperlink{https://add-architecture-draft--conmotion.netlify.app/architecture.html}{draft architecture specification}. I have not completed it on schedule due to the troubleshooting described above.
    \item For authoring the specification and preparing to develop the core of the prototype, I reviewed the sources below and modified the outline accordingly.
        \begin{itemize}
            \item transparency service specifications (e.g. \hyperlink{https://datatracker.ietf.org/doc/draft-ietf-scitt-architecture/}{SCITT}; \hyperlink{https://c2sp.org/static-ct-api}{C2SP Static Certificate Transparency API}) and industry analysis of their efficacy (for  \hyperlink{https://github.com/aj-stein/practicum_proposal/blob/main/paper.pdf}{deliverable \#3})
            \item taxonomies and models for auditing and monitoring (for  \hyperlink{https://github.com/aj-stein/practicum_proposal/blob/main/paper.pdf}{deliverable \#2})
        \end{itemize}
\end{enumerate}

\section*{Tasks for the Next Project Report}

In the next two weeks, I will focus on the following goals. I have sorted them in order of priority.

\begin{enumerate}
    \item Request feedback from advisors for the initial draft of the architecture specification.
    \item Start development of submission API for cloud service providers and external third-party auditors.
    \item Complete outline of FedRAMP critical analysis.
    \item Start draft of FedRAMP critical analysis, request feedback from advisors.
\end{enumerate}

\section*{Questions or issues I'm having}

\subsection*{Alignment with Practicum Requirements}

\begin{enumerate}
    \item My project focuses on a policy challenge in cloud security, but does not have a conventional policy recommendation like other policy track proposals. Is a policy document an explicit requirement?
\end{enumerate}

\subsection*{Project Scope}

\begin{enumerate}
    \item One of my deliverables (a critical analysis of FedRAMP's current approach) is not a prerequisite I must complete to start other deliverables, but will establish important context for readers in detail, but it will likely be too detailed for the final report. Should I include this deliverable in the final report appendix or use it as an input for a summary analysis in the final report only?
    \item One of my deliverables will be a prototype of federated data service, which will have server and client components that do statistical analysis of data. There will not be a user-friendly web interface to keep scope focused and meet the projected timeline. Is this reasonable?
    \item If I cannot complete all the code for the prototype and I must limit the scope of development, do I inventory outstanding work in the future work section of the final report? Will this negatively impact my final grade for this project. Is this normal and expected?
\end{enumerate}

\subsection*{Evaluation and Measurement}

\begin{enumerate}
    \item I am proposing a novel solution that is considerably different from the current state without an ``apples to apples" comparison. It is more like an ``apples to oranges" comparison. Neither official records from FedRAMP, similar organizations, or research indicate the existence of a solution like the one I propose. There is limited data to compare my design with the current state. Is there any significant risk to my project if I design my own quantitative and qualitative metrics?    
\end{enumerate}

\section*{Methodology Paragraph Summary}

For this project, I will use multiple methods to implement an alternative architecture for monitoring cloud services and modeling its potential impact. To start, I will use a quantitative and qualitative analysis of the current shortcomings and gaps for the current FedRAMP Continuous Monitoring Program. This will be the primary example of centralized continuous monitoring for which I design my mutual monitoring model for comparison. For qualitative analysis, I can perform textual analysis and sentiment analysis. I will leverage academic research, industry analysis, and a new primary source: FedRAMP's web-based forums for \hyperlink{https://www.fedramp.gov/20x/working-groups/}{the 20x reform initiative and its community working groups}. In these forums, stakeholders discuss their praise and criticism of current centralized processes and plans for future ones, often summarizing their pain points highly relevant to designing an alternative process. In addition, I will use publicly available information from FedRAMP and industry analysis to quantify the burden of the current FedRAMP Continuous Monitoring and its manual workflow. As I build a prototype based on my architecture, I will design several use cases to estimate the cost and resource efficiency to compare those costs against the estimated costs for my solution. In addition to these methods, I will use advisors familiar with FedRAMP from different stakeholder perspectives to validate information or analysis where these methods prove lacking and leave gaps.

\section*{Timeline}

\begin{xltabular}{\textwidth}{|l|X|l|}
    % \caption{Timeline for Mutual Monitoring Project} 
    % \label{tab:timeline} \\
    \hline \multicolumn{1}{|c|}{\textbf{Week \#}} & \multicolumn{1}{c|}{\textbf{Description of Task}} & \multicolumn{1}{c|}{\textbf{Status}} \\
    \endfirsthead
    \hline
    % \multicolumn{3}{c}%
    % {\tablename\ \thetable{} -- continued from previous page} \\
    % \hline \multicolumn{1}{|c|}{\textbf{Week \#}} & \multicolumn{1}{c|}{\textbf{Description of Task}} & \multicolumn{1}{c|}{\textbf{Status}} \\ \hline 
    % \endhead
    % \hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
    % \endfoot    
    % \hline
    % \endlastfoot
    W1 (May 12-18) & Identify references for key research topics. & Complete \\
    \hline
    W1 & Identify advisors to review FedRAMP analysis and architecture. & Complete \\
    \hline
    W2 (May 19-25) & Initialize code repository for prototype service. & Complete \\
    \hline
    W2 & Present proposal to advisors and integrate feedback; obtain commitment from advisors. & In Progress \\
    \hline
    W2 & Read FedRAMP documentation for continuous monitoring processes. & In Progress \\
    \hline
    W2 & Begin outline of FedRAMP ConMon critical analysis. & In Progress \\
    \hline
    W3 (May 26 - Jun 1) & Implement data service internals and submission API. & Pending \\
    \hline
    W3 (May 26 - Jun 1) & First draft of data service architecture specification. & Pending \\
    \hline
    W4 (June 2-8) & Implement data service internals and submission API. & Pending \\
    \hline
    W4 & Finalize architecture specification with advisors' reviews. & Pending \\
    \hline    
    W5 (June 9-15) & Implement data service client to submit to submission API instances. & Pending \\
    \hline
    W5 & Complete data service internals and submission API. & Pending \\
    \hline
    W5 & Complete FedRAMP critical analysis document. & Pending \\
    \hline
    W6 (June 16-22) & Complete data service client to submit to submission API instances. & Pending \\
    \hline
    W6 (June 16-22) & Implement continuous monitoring quantitative processing module for API. & Pending \\
    \hline
    W6 & Design MVP continuous monitoring use cases and quantitative measurements. & Pending \\
    \hline
    W7 (June 23-29) & Complete continuous monitoring quantitative processing module for API. & Pending \\
    \hline
    W7 & Implement MVP continuous monitoring use cases in API quantitative processing module. & Pending \\
    \hline    
    W8 (June 30 - July 6) & Start prototype deployment to cloud service tenants for testing. & Pending \\
    \hline
    W9 (July 7-13) & Complete prototype deployment to cloud service tenants for testing. & Pending \\
    \hline
\end{xltabular}

\section*{Evaluation}

[Include any evaluation plans and/or results by Progress Report 4. This may expand as you finalize the report.]

\section*{Report Outline}

[Include an outline of your final report by Progress Report 4. This may expand as you finalize the report.]

\nocite{*}
\bibliographystyle{apacite}
%  Relatives path work because you initialize top-level practicum repo
%  so the paths are consistent. Clone this repo and initialize the
%  submodules and it will work.
%  https://github.com/aj-stein/practicum.git
\bibliography{../references.bib}

\section*{\centering{Appendix}}

%  Relatives path work because you initialize top-level practicum repo
%  so the paths are consistent. Clone this repo and initialize the
%  submodules and it will work.
%  https://github.com/aj-stein/practicum.git
\includepdf[pages=-]{../prototype/build/architecture.pdf}

\label{appendix:outline}
\includepdf[pages=-]{../notes.pdf}

\end{document}
